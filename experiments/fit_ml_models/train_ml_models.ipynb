{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96064375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup_file: /home/monte.flora/python_packages/WoF_post/wofs/data/psadilookup.dat\n",
      "lookup_file: /home/monte.flora/python_packages/WoF_post/wofs/data/psadilookup.dat\n",
      "\n",
      "Training a new model....\n",
      "Target: all_severe \n",
      "          Model Name : XGBoost \n",
      "          Lead Time: first_hour \n",
      "          Resample: None \n",
      "          Mode: retro\n",
      "\n",
      " 10%|███████████▏                                                                                                    | 1/10 [01:48<16:16, 108.54s/trial, best loss: -0.3593506805248306]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ## Train the Real-Time ML Models \n",
    "# \n",
    "# The real-time ML models are trained on all available warm season cases (2017-current). \n",
    "# The following models are trained in this script: \n",
    "# 1. Severe Hail \n",
    "# 2. Severe Wind \n",
    "# 3. Tornado \n",
    "# 4. Sig. Hail\n",
    "# 5. Sig. Wind \n",
    "# 6. Sig. Tornado\n",
    "# 7. All-severe \n",
    "# 8. All-sig-severe \n",
    "# \n",
    "# The model classes include: \n",
    "# 1. LogisticRegression\n",
    "# 2. HistGradientBoosting \n",
    "# 3. RandomForest \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\"\"\" usage: stdbuf -oL python -u official_train_ml_models.py > & log_train_models & \"\"\"\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# The custom classifier \n",
    "import sys\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/ml_workflow')\n",
    "\n",
    "from ml_workflow import TunedEstimator \n",
    "from wofs_ml_severe import load_ml_data\n",
    "from wofs_ml_severe.common.emailer import Emailer \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from os.path import join, exists\n",
    "import os\n",
    "import itertools\n",
    "import multiprocessing as mp \n",
    "\n",
    "\n",
    "import sklearn.exceptions\n",
    "os.environ[\"PYTHONPATH\"] = os.path.dirname(sklearn.exceptions.__file__)\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::exceptions.ConvergenceWarning:sklearn.svm.base\"\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def scorer(estimator, X, y):\n",
    "    pred = estimator.predict_proba(X)[:,1]\n",
    "    return -average_precision_score(y, pred)\n",
    "\n",
    "def dates_to_groups(dates, n_splits=5): \n",
    "    \"\"\"Separated different dates into a set of groups based on n_splits\"\"\"\n",
    "    df = dates.copy()\n",
    "    df = df.to_frame()\n",
    "    \n",
    "    unique_dates = np.unique(dates.values)\n",
    "    np.random.shuffle(unique_dates)\n",
    "\n",
    "    df['groups'] = np.zeros(len(dates))\n",
    "    for i, group in enumerate(np.array_split(unique_dates, n_splits)):\n",
    "        df.loc[dates.isin(group), 'groups'] = i+1 \n",
    "        \n",
    "    groups = df.groups.values\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def get_search_space(model_name, X):\n",
    "    if model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier(n_jobs=-1, random_state=123)\n",
    "        n_features = X.shape[1]\n",
    "        search_space = {\n",
    "                'criterion' : ['gini', 'entropy'], \n",
    "                'n_estimators' : [100, 125, 150, 175, 200, 225, 250, 275, 300], \n",
    "                'max_depth' : [2,5, 10, 15, 25, 40, None],\n",
    "                'min_samples_split' : [2, 5, 10,15, 40],\n",
    "                'min_samples_leaf':  [2,4, 5,10,15,20,25, 50],\n",
    "                'max_features': list(np.arange(1, n_features)),\n",
    "                'class_weight' : ['balanced', None],\n",
    "                } \n",
    "        n_jobs = 1\n",
    "        n_iter, patience = 100, 20\n",
    "        optimizer = 'tpe'\n",
    "    \n",
    "    elif model_name == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=123)\n",
    "        search_space = {\n",
    "                'C': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "                'class_weight' : [None, 'balanced'],\n",
    "                'penalty' : ['l1', 'l2'], \n",
    "                }\n",
    "        n_jobs = 60\n",
    "        n_iter, patience = 20, 20 \n",
    "        optimizer = 'grid_search'\n",
    "    \n",
    "    elif model_name == 'XGBoost':\n",
    "        n_jobs=1\n",
    "        n_iter, patience = 10, 3\n",
    "        optimizer = 'tpe'\n",
    "        model = XGBClassifier(objective= 'binary:logistic', seed=123, \n",
    "                          tree_method='gpu_hist', gpu_id=0)\n",
    "    \n",
    "        search_space = {\n",
    "        'n_estimators' : [50, 100, 200, 300, 325],\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.5],\n",
    "        'max_depth': [3,5,6,10,15,20],\n",
    "        'subsample': list(np.arange(0.5, 1.0, 0.1)),\n",
    "        'colsample_bytree': list(np.arange(0.5, 1.0, 0.1)),\n",
    "        'gamma': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10., 100.],\n",
    "        'alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10., 100.],\n",
    "        'lambda': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10., 100., 1000., 10000.],\n",
    "        'sampling_method' : ['uniform', 'gradient_based'],\n",
    "        'min_child_weight' : list(np.arange(1, 8, 1, dtype=int)),    \n",
    "    }\n",
    "        \n",
    "    return model, search_space, n_jobs, n_iter, patience, optimizer\n",
    "\n",
    "\n",
    "def get_feature_type(X, categorical_features):\n",
    "    \n",
    "    # Define the categorical features for the pre-processing pipeline. \n",
    "    numeric_features = [i for i in range(len(X.columns))]\n",
    "    categorical_features = [list(X.columns).index(f) for f in categorical_features]\n",
    "    _ = [numeric_features.remove(i) for i in categorical_features]\n",
    "    \n",
    "    return categorical_features, numeric_features \n",
    "\n",
    "\n",
    "def get_target_str(target):\n",
    "    # Initialize the kwargs for the hyperparameter optimization.\n",
    "    if isinstance(target, list):\n",
    "        if 'sig_severe' in target[0]:\n",
    "            target = 'all_sig_severe'\n",
    "        else:\n",
    "            target = 'all_severe'\n",
    "   \n",
    "    return target \n",
    "        \n",
    "\n",
    "# In[5]:\n",
    "OUT_PATH = '/work/mflora/ML_DATA/NEW_ML_MODELS'\n",
    "\n",
    "#  ['wind_severe_0km', 'hail_severe_0km', 'tornado_severe_0km'],\n",
    "# ['wind_sig_severe_0km', 'hail_sig_severe_0km', 'tornado_sig_severe_0km']\n",
    "\n",
    "# Notes:\n",
    "# For the first round of experiments, recreate the models from the 2021 paper. \n",
    "\n",
    "\"\"\"\n",
    "target_cols = ['wind_severe_0km', \n",
    "               'hail_severe_0km', \n",
    "               'tornado_severe_0km', \n",
    "               'wind_sig_severe_0km', \n",
    "               'hail_sig_severe_0km', \n",
    "               'tornado_sig_severe_0km',\n",
    "               ['wind_severe_0km', 'hail_severe_0km', 'tornado_severe_0km'],\n",
    "               ['wind_sig_severe_0km', 'hail_sig_severe_0km', 'tornado_sig_severe_0km'],\n",
    "              ]\n",
    "\n",
    "model_names = ['XGBoost', 'LogisticRegression', 'RandomForest']\n",
    "resampling = [None]\n",
    "times = ['first_hour', 'second_hour', 'third_hour', 'fourth_hour']\n",
    "# If 'training', it will load the data with the original training dates from Flora et al. (2021, MWR)\n",
    "# If None, train the operational models. \n",
    "modes = [None]\n",
    "\n",
    "\"\"\"\n",
    "target_cols = [['wind_severe_0km', 'hail_severe_0km', 'tornado_severe_0km']]\n",
    "model_names = ['XGBoost']\n",
    "resampling = [None]\n",
    "times = ['first_hour'] \n",
    "modes = ['training'] \n",
    "\n",
    "\n",
    "# If True, existing files will be overwritten; setting to False is useful when the scripts\n",
    "# fails to finish and I don't want to redo stuff. \n",
    "overwrite = True\n",
    "emailer = Emailer()\n",
    "\n",
    "for target, model_name, time, resample, mode in itertools.product(target_cols, \n",
    "                                                            model_names,\n",
    "                                                            times,\n",
    "                                                            resampling, \n",
    "                                                                  modes): \n",
    "   \n",
    "    def fitting(): \n",
    "        retro_str = 'retro' if mode == 'training' else 'realtime'\n",
    "    \n",
    "        target_str = get_target_str(target)\n",
    "    \n",
    "        fname = join(OUT_PATH, \n",
    "                 f'{model_name}_{target_str}_{resample}_{time}_{retro_str}.joblib')\n",
    "    \n",
    "        if not overwrite:\n",
    "            if exists(fname):\n",
    "                return None \n",
    "            \n",
    "        print('\\nTraining a new model....')\n",
    "        subject = f\"\"\"Target: {target_str} \n",
    "          Model Name : {model_name} \n",
    "          Lead Time: {time} \n",
    "          Resample: {resample} \n",
    "          Mode: {retro_str}\\n\"\"\"\n",
    "        print(subject)\n",
    "    \n",
    "        start_time = emailer.get_start_time()\n",
    "\n",
    "        # Load the data. Using the run dates, we can group the data into \n",
    "        # 5 cross-validation folds, which will be used for hyperparameter optimization\n",
    "        # and training the calibration model. \n",
    "        X, y, metadata = load_ml_data(target_col=target, \n",
    "                                  lead_time=time,\n",
    "                                  mode=mode,\n",
    "                                 )\n",
    "        \n",
    "        # Dropping the 'area', 'minor_axis_length', 'major_axis_length' and 'Run Date'\n",
    "        drop_vars = ['area', 'minor_axis_length', 'major_axis_length', 'Run Date']\n",
    "        X.drop(drop_vars, axis=1, inplace=True)\n",
    "        \n",
    "        dates = metadata['Run Date']\n",
    "        groups = dates_to_groups(dates, n_splits=5)\n",
    "    \n",
    "        model, search_space, n_jobs, n_iter, patience, optimizer = get_search_space(model_name, X)\n",
    "    \n",
    "        categorical_features, numeric_features = get_feature_type(X, categorical_features=['Initialization Time'])\n",
    "    \n",
    "        # Initialize the cross-validation groups \n",
    "        cv = list(GroupKFold(n_splits=5).split(X,y,groups))\n",
    "    \n",
    "        output_fname = join(OUT_PATH, 'hyperopt_results', \n",
    "                        f'{model_name}_{target_str}_{resample}_{time}_{retro_str}.feather')\n",
    "    \n",
    "        # Since logistic regression has a small search space, we can use simple GridSearchCV\n",
    "        # for the hyperparameter search. \n",
    "        scorer_ = 'average_precision' if model_name == \"LogisticRegression\" else scorer\n",
    "        \n",
    "        hyperopt_kwargs = {'search_space' : search_space, \n",
    "                   'optimizer' : optimizer, \n",
    "                   'max_evals' : n_iter, \n",
    "                   'patience' : patience, \n",
    "                  'scorer' : scorer_, \n",
    "                  'n_jobs' : n_jobs, \n",
    "                  'cv' : cv, \n",
    "                  'output_fname' : output_fname \n",
    "                      }\n",
    "    \n",
    "        # Initialize the kwargs for the Pipeline. \n",
    "        pipeline_kwargs={'imputer' : 'simple', \n",
    "                     'resample': resample, \n",
    "                     'scaler': 'standard', \n",
    "                     'numeric_features' : numeric_features, \n",
    "                     'categorical_features' :  categorical_features}\n",
    "    \n",
    "        # Initialize the kwargs for the calibration model. \n",
    "        calibration_cv_kwargs = {'method' : 'isotonic', 'ensemble' : False, \n",
    "                         'cv' : cv, 'n_jobs': n_jobs}\n",
    "\n",
    "        # Fit the model and save it. \n",
    "        estimator = TunedEstimator(model, pipeline_kwargs, hyperopt_kwargs, calibration_cv_kwargs)\n",
    "    \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "    \n",
    "        estimator.fit(X,y,groups)\n",
    "        estimator.save(fname)\n",
    "        del estimator\n",
    "        #save the model here on the disk\n",
    "\n",
    "        try:\n",
    "            emailer.send_email(subject, start_time)\n",
    "        except:\n",
    "            print('Unable to send email. Possibly an NSSL network issue.')\n",
    "\n",
    "    fitting_process = mp.Process(target=fitting)\n",
    "    fitting_process.start()\n",
    "    fitting_process.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
