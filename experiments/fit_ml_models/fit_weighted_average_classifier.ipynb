{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e048541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lookup_file: /home/monte.flora/python_packages/WoF_post/wofs/data/psadilookup.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "# The custom classifier \n",
    "import sys, os\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/ml_workflow')\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/WoF_post')\n",
    "\n",
    "from wofs.ml.load_ml_models import load_ml_model\n",
    "from wofs.post.utils import load_yaml\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from ml_workflow.weighted_average_classifier import WeightedAverageClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from ml_workflow.ml_methods import brier_skill_score\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c552dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def dates_to_groups(dates, n_splits=5): \n",
    "    \"\"\"Separated different dates into a set of groups based on n_splits\"\"\"\n",
    "    df = dates.copy()\n",
    "    df = df.to_frame()\n",
    "    \n",
    "    unique_dates = np.unique(dates.values)\n",
    "    np.random.shuffle(unique_dates)\n",
    "\n",
    "    df['groups'] = np.zeros(len(dates))\n",
    "    for i, group in enumerate(np.array_split(unique_dates, n_splits)):\n",
    "        df.loc[dates.isin(group), 'groups'] = i+1 \n",
    "        \n",
    "    groups = df.groups.values\n",
    "    \n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f1551",
   "metadata": {},
   "source": [
    "### Fit the Official Weighted Average Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d7340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_best_val_score(model_name, target, time, \n",
    "                          resample = None, retro_str = 'realtime'): \n",
    "    \"\"\"Return the best cross-validation score from the hyperparam tuning. \"\"\"\n",
    "    BASE_PATH =  '/work/mflora/ML_DATA/NEW_ML_MODELS/hyperopt_results'\n",
    "    \n",
    "    fname = os.path.join(BASE_PATH, \n",
    "                    f'{model_name}_{target}_{resample}_{time}_{retro_str}.feather')\n",
    "\n",
    "    df = pd.read_feather(fname)\n",
    "    ascending = False if model_name == \"LogisticRegression\" else True\n",
    "    df_sorted = df.sort_values(by='loss', ascending=ascending)['loss']\n",
    "    \n",
    "    df_sorted.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    val = df_sorted[0]\n",
    "    \n",
    "    if val < 0:\n",
    "        return -val\n",
    "    else:\n",
    "        return val \n",
    "    \n",
    "def get_weights(model_names, time, target):\n",
    "    \"\"\"Compute the weights for the weighted averaging.\"\"\"\n",
    "    scores = [return_best_val_score(model_name, target, time) for model_name in model_names]\n",
    "    total_scores = np.sum(scores)\n",
    "    return scores / total_scores \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e48a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hazard: WIND_SEVERE_0KM....Time : FIRST_HOUR\n",
      "Classifier Weights: [(0.5339227974332283, 'LogisticRegression'), (0.4660772025667717, 'XGBoost')]\n",
      "\n",
      " Hazard: WIND_SEVERE_0KM....Time : SECOND_HOUR\n",
      "Classifier Weights: [(0.525522062041453, 'LogisticRegression'), (0.4744779379585469, 'XGBoost')]\n",
      "\n",
      " Hazard: HAIL_SEVERE_0KM....Time : FIRST_HOUR\n",
      "Classifier Weights: [(0.5132133180839746, 'LogisticRegression'), (0.48678668191602537, 'XGBoost')]\n",
      "\n",
      " Hazard: HAIL_SEVERE_0KM....Time : SECOND_HOUR\n",
      "Classifier Weights: [(0.5036722983105061, 'LogisticRegression'), (0.4963277016894938, 'XGBoost')]\n",
      "\n",
      " Hazard: TORNADO_SEVERE_0KM....Time : FIRST_HOUR\n",
      "Classifier Weights: [(0.48758181467755374, 'LogisticRegression'), (0.5124181853224462, 'XGBoost')]\n",
      "\n",
      " Hazard: TORNADO_SEVERE_0KM....Time : SECOND_HOUR\n",
      "Classifier Weights: [(0.4968198361565161, 'LogisticRegression'), (0.503180163843484, 'XGBoost')]\n",
      "\n",
      " Hazard: ALL_SEVERE....Time : FIRST_HOUR\n",
      "Classifier Weights: [(0.5108755001314864, 'LogisticRegression'), (0.48912449986851364, 'XGBoost')]\n",
      "\n",
      " Hazard: ALL_SEVERE....Time : SECOND_HOUR\n",
      "Classifier Weights: [(0.503127558892854, 'LogisticRegression'), (0.496872441107146, 'XGBoost')]\n",
      "\n",
      " Hazard: ALL_SIG_SEVERE....Time : FIRST_HOUR\n",
      "Classifier Weights: [(0.4912231028907625, 'LogisticRegression'), (0.5087768971092375, 'XGBoost')]\n",
      "\n",
      " Hazard: ALL_SIG_SEVERE....Time : SECOND_HOUR\n",
      "Classifier Weights: [(0.478336154405071, 'LogisticRegression'), (0.521663845594929, 'XGBoost')]\n"
     ]
    }
   ],
   "source": [
    "from wofs_ml_severe.io.load_ml_models import load_ml_model\n",
    "\n",
    "times = ['first_hour', 'second_hour']\n",
    "config_path = '/home/monte.flora/python_packages/wofs_ml_severe/wofs_ml_severe/conf/ml_config_realtime.yml'\n",
    "targets = ['wind_severe_0km', 'hail_severe_0km', 'tornado_severe_0km', 'all_severe', 'all_sig_severe']\n",
    "model_names = ['LogisticRegression', 'XGBoost'] \n",
    "\n",
    "ml_config = load_yaml(config_path)\n",
    "OUT_PATH = '/work/mflora/ML_DATA/OPERATIONAL_MODELS'  \n",
    "retro_str = 'realtime'\n",
    "    \n",
    "for target, time in itertools.product(targets, times):\n",
    "    print(f'\\n Hazard: {target.upper()}....Time : {time.upper()}')\n",
    "    estimators = [] \n",
    "    for model_name in model_names: \n",
    "        parameters = {\n",
    "                'target' : target,\n",
    "                'time' : time, \n",
    "                'drop_opt' : '',\n",
    "                'model_name' : model_name,\n",
    "                'ml_config' : ml_config,\n",
    "            }\n",
    "    \n",
    "    \n",
    "        model_dict = load_ml_model(**parameters)\n",
    "        model = model_dict['model']\n",
    "        features = model_dict['X'].columns\n",
    "\n",
    "        estimators.append(model)\n",
    "\n",
    "    # Fit the WeightedAverageClassifier\n",
    "    weights = get_weights(model_names, time, target)\n",
    "    clf = WeightedAverageClassifier(estimators,  weights=weights)\n",
    "    clf.features = features\n",
    "    clf.save(os.path.join(OUT_PATH, f'Average_{target}_None_{time}_{retro_str}.joblib'))\n",
    "    \n",
    "    print(f'Classifier Weights: {list(zip(clf.weights_, model_names))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a98154f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/mflora/ML_DATA/NEW_ML_MODELS/Average_all_sig_severe_None_second_hour_realtime.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check that we can successfully load the weighted average classifier. \u001b[39;00m\n\u001b[1;32m      2\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m : target,\n\u001b[1;32m      4\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m : time, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_config\u001b[39m\u001b[38;5;124m'\u001b[39m : ml_config,\n\u001b[1;32m      8\u001b[0m             }\n\u001b[0;32m---> 10\u001b[0m model_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_ml_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m model_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/python_packages/wofs_ml_severe/wofs_ml_severe/io/load_ml_models.py:30\u001b[0m, in \u001b[0;36mload_ml_model\u001b[0;34m(retro, **parameters)\u001b[0m\n\u001b[1;32m     27\u001b[0m resample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m     28\u001b[0m model_fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretro_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_fname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/work/mflora/miniconda3/envs/wofs_post/lib/python3.8/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/mflora/ML_DATA/NEW_ML_MODELS/Average_all_sig_severe_None_second_hour_realtime.joblib'"
     ]
    }
   ],
   "source": [
    "# Check that we can successfully load the weighted average classifier. \n",
    "parameters = {\n",
    "                'target' : target,\n",
    "                'time' : time, \n",
    "                'drop_opt' : '',\n",
    "                'model_name' : 'Average',\n",
    "                'ml_config' : ml_config,\n",
    "            }\n",
    "    \n",
    "model_dict = load_ml_model(**parameters)\n",
    "model_dict['features']\n",
    "model_dict['model']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
