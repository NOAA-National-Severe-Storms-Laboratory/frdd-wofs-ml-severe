{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ee9b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import join, basename\n",
    "from glob import glob\n",
    "import joblib\n",
    "from collections import ChainMap\n",
    "import traceback\n",
    "\n",
    "class DateTimeIterator:\n",
    "    def __init__(self, base_path='/work/mflora/SummaryFiles'):\n",
    "        self.base_path = base_path \n",
    "        self.base_wrfout_path = '/work/wof/realtime/FCST/'\n",
    "        \n",
    "        dates = self.get_date_dirs()\n",
    "        \n",
    "        self._paths = []\n",
    "        for d in dates:\n",
    "            path = join(self.base_path, d)\n",
    "            init_times = self.get_init_time_dirs(d)\n",
    "            paths = [join(path, t) for t in init_times] \n",
    "            for p in paths:\n",
    "                self._paths.append(p) \n",
    "        \n",
    "        self.index=0\n",
    "    \n",
    "    def get_date_dirs(self):\n",
    "        return [d for d in os.listdir(self.base_path) if 'txt' not in d]\n",
    "    \n",
    "    def get_init_time_dirs(self, date):\n",
    "        return [t for t in os.listdir(join(self.base_path, date)) if 'base' not in t] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        try:\n",
    "            result = self._paths[self.index]\n",
    "        except IndexError:\n",
    "            raise StopIteration\n",
    "        self.index+=1\n",
    "        return result\n",
    "\n",
    "\n",
    "class SummaryFileValidater:\n",
    "    \"\"\"\n",
    "    SummaryFileValidater checks for empty directories, missing summary files, \n",
    "    and corrupted/missing WRFOUT files. The first task is to determine the number of WRFOUT\n",
    "    files per date and initialization time. In this first task, we determine  \n",
    "    corrupted files as situations where the number of files are inconsistent for the \n",
    "    different ensemble members (At the moment, in cases where the file exists, \n",
    "    this code does not explicitly check whether files are truly corrupted; i.e., unopenable).\n",
    "    In the case that the WRFOUT path is empty, we know that the corresponding summary file path \n",
    "    is also empty and therefore it can be deleted. \n",
    "    As for missing files, we assess when the summary file count (for a given summary file type)\n",
    "    differs from the WRFOUT count. \n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    base_path : path-like \n",
    "    \n",
    "    n_jobs : int \n",
    "        Number of processors for parallelization. \n",
    "    \n",
    "    Attributes\n",
    "    --------------------\n",
    "    empty_paths : list of paths \n",
    "        Empty directories \n",
    "        \n",
    "    missing_files : list of paths \n",
    "        Missing summary files. \n",
    "        \n",
    "    corrupted_files : list of paths \n",
    "        Corrupted WRFOUT files. \n",
    "    \"\"\"\n",
    "    SUMMARY_FILE_TYPES = ['ENS', 'SVR', 'ENV', '30M', '60M', 'SND', 'SWT']\n",
    "    \n",
    "    RLT_DATES = ['20170509', \n",
    "                 '20170517',\n",
    "                 '20170527',\n",
    "                 '20170518',\n",
    "                 '20170516',\n",
    "                 '20170523']\n",
    "    \n",
    "    def __init__(self, base_path = '/work/mflora/SummaryFiles', n_jobs=30 ):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.base_path = base_path\n",
    "        self._empty_paths = []\n",
    "        self._missing_files = {t : [] for t in self.SUMMARY_FILE_TYPES}  \n",
    "        self._corrupted_files = [] \n",
    "    \n",
    "    def __call__(self):\n",
    "        # For a summary file path, get the count of WRFOUT files.\n",
    "        # When the corresponding WRFOUT file path is empty, then\n",
    "        # we know the summary file path is empty and thus it can be \n",
    "        # removed. We can also use to determine if there are missing files. \n",
    "        self.wrf_file_count = self.get_num_wrf_files()\n",
    "        self._remove_empty_paths()\n",
    "        # In case not all empty paths are removed, \n",
    "        # we need to keep track of those paths. \n",
    "        self._find_empty_paths\n",
    "        self._find_missing_files()\n",
    "        \n",
    "    @property\n",
    "    def empty_paths(self):\n",
    "        \"\"\"The empty_paths property.\"\"\"\n",
    "        return self._empty_paths\n",
    "    \n",
    "    @property\n",
    "    def missing_files(self):\n",
    "        \"\"\"The missing files property.\"\"\"\n",
    "        return self._missing_files\n",
    "    \n",
    "    @property\n",
    "    def corrupted_files(self):\n",
    "        \"\"\"The corrupted files property.\"\"\"\n",
    "        return self._corrupted_files\n",
    "    \n",
    "    def _appender(self, attr, paths, file_type=None):\n",
    "        if isinstance(paths, list):\n",
    "            if file_type is not None:\n",
    "                attr[file_type].extend(paths)\n",
    "            else:\n",
    "                attr.extend(paths)\n",
    "        else:\n",
    "            if file_type is not None:\n",
    "                attr[file_type].append(paths)\n",
    "            else:\n",
    "                attr.append(paths)\n",
    "    \n",
    "    @empty_paths.setter\n",
    "    def empty_paths(self, paths):\n",
    "        self._appender(self._empty_paths, paths)\n",
    "    \n",
    "    @missing_files.setter\n",
    "    def missing_files(self, arg):\n",
    "        self._appender(self._missing_files, arg[0], arg[1])\n",
    "    \n",
    "    @corrupted_files.setter\n",
    "    def corrupted_files(self, paths):\n",
    "        self._appender(self._corrupted_files, paths)\n",
    "    \n",
    "    def _is_dir_empty(self, path):\n",
    "        return not any(os.scandir(path))\n",
    "\n",
    "    def _find_empty_paths(self,):\n",
    "        \"\"\"Find empty directories\"\"\"\n",
    "        iterator = DateTimeIterator()\n",
    "        \n",
    "        def worker(path):\n",
    "            if self._is_dir_empty(path):\n",
    "                return path \n",
    "            return None\n",
    "    \n",
    "        results = joblib.Parallel(n_jobs = self.n_jobs,\n",
    "                        backend='loky',\n",
    "                        verbose=0)(joblib.delayed(worker)(path) for path in iterator)\n",
    "        \n",
    "        for path in results:\n",
    "            if path is not None:\n",
    "                self.empty_paths = path\n",
    "    \n",
    "    def _remove_empty_paths(self,):\n",
    "        \"\"\" Remove empty directories that are not associated with \n",
    "        missing and/or corrupt files\"\"\"\n",
    "        wrf_file_count = self.get_num_wrf_files()\n",
    "        for path, count in wrf_file_count.items():\n",
    "            # The WRFOUTs for this date and init time are \n",
    "            # empty, so the path can be removed because \n",
    "            # it is empty as well. \n",
    "            if count == 0: \n",
    "                try:\n",
    "                    os.rmdir(path)\n",
    "                except Exception:\n",
    "                    print(f'Unable to remove {path}. Likely permission issues or was not an empty dir!')\n",
    "                    print(traceback.format_exc())\n",
    "        \n",
    "    def _find_missing_files(self,): \n",
    "        \"\"\" Find the missing files. \"\"\"\n",
    "        iterator = DateTimeIterator()\n",
    "        \n",
    "        def worker(path, file_type): \n",
    "            # The path is empty.\n",
    "            if path in self.empty_paths:\n",
    "                return None\n",
    "            # Another check of an empty path. \n",
    "            elif self.wrf_file_count[path] == 0:\n",
    "                return None \n",
    "            # The WRFOUTs are inconsistent amongst the ensemble members\n",
    "            elif self.wrf_file_count[path] == -1:\n",
    "                return None\n",
    "            \n",
    "            files = glob(join(path, f'wofs_{file_type}_*'))\n",
    "            if not self._check_file_count(path, file_type, files):\n",
    "                # Check that the number of files is correct based \n",
    "                # on the WRFOUTs.\n",
    "                return path\n",
    "            return None  \n",
    "        \n",
    "        for file_type in self.SUMMARY_FILE_TYPES:\n",
    "        \n",
    "            paths = joblib.Parallel(n_jobs = self.n_jobs,\n",
    "                        backend='loky',\n",
    "                        verbose=0)(joblib.delayed(worker)(path, file_type) for path in iterator)\n",
    "        \n",
    "            for path in paths:\n",
    "                if path is not None:\n",
    "                    self.missing_files = (path, file_type) \n",
    "        \n",
    "    def _check_file_count(self, path, file_type, files):\n",
    "        \"\"\"Check the correct of summary files within a given directory exists.\n",
    "        For half-hour (top-of-the-hour) forecasts, there should 36 (72) files \"\"\"\n",
    "        n_files = len(files)\n",
    "        true_count = self.wrf_file_count[path ]\n",
    "        \n",
    "        if file_type == '60M':\n",
    "            return true_count == int(true_count/12)\n",
    "        elif file_type == '30M':\n",
    "            return n_files in [true_count - (6+1), true_count - 6] \n",
    "        else:\n",
    "            return n_files == true_count \n",
    "        \n",
    "    def get_num_wrf_files(self,):\n",
    "        \"\"\"For each date and initialization time, build a dictionary \n",
    "        determining if the number of forecast outputs per ensemble members is consistent\n",
    "        and storing that number \"\"\"\n",
    "        iterator = DateTimeIterator()\n",
    "        \n",
    "        def worker(path):\n",
    "            wrf_file_count = {}\n",
    "            path = os.path.normpath(path)\n",
    "            date, init_time = path.split(os.sep)[-2:]\n",
    "            year = date[:4]\n",
    "            \n",
    "            if date in os.listdir('/work/nusrat.yussouf/HMT/FCST/'):\n",
    "                _basePath = join('/work/nusrat.yussouf/HMT/FCST/', date, init_time)\n",
    "            elif date in os.listdir('/scratch/wof/realtime/FCST/compressed/FCST/'):\n",
    "                _basePath = join('/scratch/wof/realtime/FCST/compressed/FCST/', date, init_time)\n",
    "            else:\n",
    "                if date in self.RLT_DATES:\n",
    "                    _basePath = join('/work/wof/realtime/FCST/', year, date, 'RLT', init_time)\n",
    "                else:\n",
    "                    _basePath = join('/work/wof/realtime/FCST/', year, date, init_time)\n",
    "        \n",
    "            paths = glob(join(_basePath, 'ENS_MEM*'))\n",
    "            \n",
    "            \n",
    "            if int(year) < 2019:\n",
    "                lens = [len(glob(join(p,'wrfout*'))) for p in paths]\n",
    "            else:\n",
    "                lens = [len(glob(join(p,'wrfwof*'))) for p in paths]\n",
    "            \n",
    "            # Check that each ensemble member has the same number of forecast outputs. \n",
    "            if len(lens) > 0:\n",
    "                fcst_length_consistency =  lens.count(lens[0]) == len(lens)\n",
    "                if fcst_length_consistency:\n",
    "                    wrf_file_count[path] = lens[0]\n",
    "                else:\n",
    "                    wrf_file_count[path] = -1\n",
    "            else:\n",
    "                wrf_file_count[path] = 0\n",
    "            \n",
    "            return wrf_file_count\n",
    "        \n",
    "        wrf_file_count = joblib.Parallel(n_jobs = self.n_jobs,\n",
    "                        backend='loky',\n",
    "                        verbose=0)(joblib.delayed(worker)(path) for path in iterator)\n",
    "        \n",
    "        # Turn list of dicts into a single dict. \n",
    "        wrf_file_count = dict(ChainMap(*wrf_file_count))\n",
    "\n",
    "        return wrf_file_count \n",
    "    \n",
    "    def _find_corrupted_files(self,):\n",
    "        pass\n",
    "        #TODO: Actually determine corrupt WRFOUT files \n",
    "        # by checking if they can be opened or if they \n",
    "        # are much smaller in size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ae8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the missing files. \n",
    "validater = SummaryFileValidater()\n",
    "validater()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cba21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('summary_file_validater.pkl', 'wb') as f:\n",
    "    pickle.dump(validater, f,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed6ba5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENS': ['/work/mflora/SummaryFiles/20180620/1830',\n",
       "  '/work/mflora/SummaryFiles/20170608/2300',\n",
       "  '/work/mflora/SummaryFiles/20170608/0200',\n",
       "  '/work/mflora/SummaryFiles/20170608/1900',\n",
       "  '/work/mflora/SummaryFiles/20170608/0300',\n",
       "  '/work/mflora/SummaryFiles/20170608/2200',\n",
       "  '/work/mflora/SummaryFiles/20170608/2000',\n",
       "  '/work/mflora/SummaryFiles/20170608/0100',\n",
       "  '/work/mflora/SummaryFiles/20170608/0000',\n",
       "  '/work/mflora/SummaryFiles/20170608/2130',\n",
       "  '/work/mflora/SummaryFiles/20170608/0030',\n",
       "  '/work/mflora/SummaryFiles/20170608/0130',\n",
       "  '/work/mflora/SummaryFiles/20170608/2030',\n",
       "  '/work/mflora/SummaryFiles/20170608/2230',\n",
       "  '/work/mflora/SummaryFiles/20170608/0230',\n",
       "  '/work/mflora/SummaryFiles/20170608/2330',\n",
       "  '/work/mflora/SummaryFiles/20180618/1830',\n",
       "  '/work/mflora/SummaryFiles/20180712/1830',\n",
       "  '/work/mflora/SummaryFiles/20180718/1830',\n",
       "  '/work/mflora/SummaryFiles/20170605/0030',\n",
       "  '/work/mflora/SummaryFiles/20170605/2130',\n",
       "  '/work/mflora/SummaryFiles/20170605/2030',\n",
       "  '/work/mflora/SummaryFiles/20170605/0130',\n",
       "  '/work/mflora/SummaryFiles/20170605/2230',\n",
       "  '/work/mflora/SummaryFiles/20170605/2330',\n",
       "  '/work/mflora/SummaryFiles/20170605/0230',\n",
       "  '/work/mflora/SummaryFiles/20170605/0200',\n",
       "  '/work/mflora/SummaryFiles/20170605/2300',\n",
       "  '/work/mflora/SummaryFiles/20170605/1900',\n",
       "  '/work/mflora/SummaryFiles/20170605/2200',\n",
       "  '/work/mflora/SummaryFiles/20170605/0300',\n",
       "  '/work/mflora/SummaryFiles/20170605/0100',\n",
       "  '/work/mflora/SummaryFiles/20170605/2000',\n",
       "  '/work/mflora/SummaryFiles/20170605/2100',\n",
       "  '/work/mflora/SummaryFiles/20170605/0000',\n",
       "  '/work/mflora/SummaryFiles/20180720/1830',\n",
       "  '/work/mflora/SummaryFiles/20180627/1830',\n",
       "  '/work/mflora/SummaryFiles/20180429/0300',\n",
       "  '/work/mflora/SummaryFiles/20180630/1830',\n",
       "  '/work/mflora/SummaryFiles/20170511/2100',\n",
       "  '/work/mflora/SummaryFiles/20180623/1830',\n",
       "  '/work/mflora/SummaryFiles/20180629/1830',\n",
       "  '/work/mflora/SummaryFiles/20180716/1830',\n",
       "  '/work/mflora/SummaryFiles/20170606/2300',\n",
       "  '/work/mflora/SummaryFiles/20170606/0200',\n",
       "  '/work/mflora/SummaryFiles/20170606/1900',\n",
       "  '/work/mflora/SummaryFiles/20170606/0300',\n",
       "  '/work/mflora/SummaryFiles/20170606/2200',\n",
       "  '/work/mflora/SummaryFiles/20170606/2000',\n",
       "  '/work/mflora/SummaryFiles/20170606/0100',\n",
       "  '/work/mflora/SummaryFiles/20170606/0000',\n",
       "  '/work/mflora/SummaryFiles/20170606/2100',\n",
       "  '/work/mflora/SummaryFiles/20170606/2130',\n",
       "  '/work/mflora/SummaryFiles/20170606/0030',\n",
       "  '/work/mflora/SummaryFiles/20170606/0130',\n",
       "  '/work/mflora/SummaryFiles/20170606/2030',\n",
       "  '/work/mflora/SummaryFiles/20170606/2230',\n",
       "  '/work/mflora/SummaryFiles/20170606/0230',\n",
       "  '/work/mflora/SummaryFiles/20170606/2330',\n",
       "  '/work/mflora/SummaryFiles/20180711/1830',\n",
       "  '/work/mflora/SummaryFiles/20180527/1830',\n",
       "  '/work/mflora/SummaryFiles/20180624/1830',\n",
       "  '/work/mflora/SummaryFiles/20180719/1830',\n",
       "  '/work/mflora/SummaryFiles/20180713/1830',\n",
       "  '/work/mflora/SummaryFiles/20180621/1830',\n",
       "  '/work/mflora/SummaryFiles/20180619/1830',\n",
       "  '/work/mflora/SummaryFiles/20180709/1830',\n",
       "  '/work/mflora/SummaryFiles/20170607/0030',\n",
       "  '/work/mflora/SummaryFiles/20170607/2130',\n",
       "  '/work/mflora/SummaryFiles/20170607/2030',\n",
       "  '/work/mflora/SummaryFiles/20170607/0130',\n",
       "  '/work/mflora/SummaryFiles/20170607/2230',\n",
       "  '/work/mflora/SummaryFiles/20170607/2330',\n",
       "  '/work/mflora/SummaryFiles/20170607/0230',\n",
       "  '/work/mflora/SummaryFiles/20170607/0200',\n",
       "  '/work/mflora/SummaryFiles/20170607/2300',\n",
       "  '/work/mflora/SummaryFiles/20170607/2200',\n",
       "  '/work/mflora/SummaryFiles/20170607/0300',\n",
       "  '/work/mflora/SummaryFiles/20170607/1900',\n",
       "  '/work/mflora/SummaryFiles/20170607/0100',\n",
       "  '/work/mflora/SummaryFiles/20170607/2000',\n",
       "  '/work/mflora/SummaryFiles/20170607/2100',\n",
       "  '/work/mflora/SummaryFiles/20170607/0000',\n",
       "  '/work/mflora/SummaryFiles/20180710/1830',\n",
       "  '/work/mflora/SummaryFiles/20180625/1830',\n",
       "  '/work/mflora/SummaryFiles/20180628/1830',\n",
       "  '/work/mflora/SummaryFiles/20180622/1830',\n",
       "  '/work/mflora/SummaryFiles/20180717/1830'],\n",
       " 'SVR': [],\n",
       " 'ENV': [],\n",
       " '30M': [],\n",
       " '60M': [],\n",
       " 'SND': [],\n",
       " 'SWT': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('summary_file_validater.pkl', 'rb') as f:\n",
    "    validater = pickle.load(f)\n",
    "    \n",
    "validater.missing_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a3cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
