{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef1c223",
   "metadata": {},
   "source": [
    "### Generate the Official cb-WoFS Explainability Graphics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2751464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monte.flora/python_packages/MontePython/monte_python/object_quality_control.py:12: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(fastmath=True, parallel=False)\n",
      "/home/monte.flora/python_packages/MontePython/monte_python/object_quality_control.py:39: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit(fastmath=True,parallel=False)\n",
      "Failed to add gpm alias to meters.\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interpret-ml not installed\n",
      "scikeras is not installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 11:17:37.074687: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 11:17:37.100477: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 11:17:37.100507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 11:17:37.101459: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 11:17:37.106819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 11:17:37.998209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-05 11:17:39.514013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 877 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:21:00.0, compute capability: 8.0\n",
      "2024-03-05 11:17:39.515886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78666 MB memory:  -> device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e1:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# Box and Whisker plot for the top 5 predictors \n",
    "# pink line for a given example. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from display_names import to_display_name, to_units\n",
    "import sys\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/frdd-wofs-post')\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/wofs_ml_severe')\n",
    "\n",
    "from wofs_ml_severe import MLTrainer\n",
    "from wofs_ml_severe.io.io import HailSizeLoader\n",
    "from wofs.post.utils import load_yaml\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18fa0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json \n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "class cbWoFSExplainabilityGraphics:\n",
    "    def __init__(self, X_train, y_train):\n",
    "        \n",
    "        # Only get where the examples are matched to reports or hail size\n",
    "        # is greater than zero. \n",
    "        inds = np.where(y_train>0)[0]\n",
    "    \n",
    "        X_train_subset = X_train.iloc[inds, :]\n",
    "        X_train_subset.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        #X_train_subset['mid_level_lapse_rate_ens_mean_spatial_mean'] /= -2.7\n",
    "        #X_train_subset['low_level_lapse_rate_ens_mean_spatial_mean'] /= -3.0\n",
    "        \n",
    "        # Convert mid-level temps?  \n",
    "        \n",
    "        self.X_train = X_train_subset\n",
    "        self.features = X_train_subset.columns \n",
    "        \n",
    "        self.max_min_val_dict = { }\n",
    "        \n",
    "    def get_order_of_magnitude(self, value):\n",
    "        return int(math.log10(abs(value)))\n",
    "        \n",
    "    def get_fontsize(self, value):\n",
    "        \"\"\"Get the fontsize based on order of magnitude.\"\"\"\n",
    "        try:\n",
    "            oom = self.get_order_of_magnitude(value)\n",
    "        except ValueError:\n",
    "            return 10\n",
    "            \n",
    "        if oom <= -1:\n",
    "            return 7\n",
    "    \n",
    "        elif oom > 2: \n",
    "            return 7\n",
    "    \n",
    "        else:\n",
    "            return 10 \n",
    "    \n",
    "    def is_negatively_oriented(self, values):\n",
    "        \"\"\"Check if the value is negatively oriented (higher negative values are meaningful)\"\"\"\n",
    "        # If the absolute value of the minimum is larger than the \n",
    "        # maximum value, then the values are negatively oriented. \n",
    "        if abs(np.min(values)) > np.max(values):\n",
    "            return True\n",
    "        else: \n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def _round(self, value, mode):\n",
    "\n",
    "        def round_to_nearest_fifth(x):    \n",
    "            if x < 0:\n",
    "                return -round_to_nearest_fifth(-x)\n",
    "            elif x < 10:\n",
    "                return x\n",
    "            else:\n",
    "                if mode=='upper': \n",
    "                    return math.ceil(x / 5,) * 5\n",
    "                else:\n",
    "                    return math.floor(x / 5,) * 5\n",
    "        \n",
    "        if value == 0.0:\n",
    "            return 0.0, 0\n",
    "    \n",
    "        # Find the order of magnitude (oom)\n",
    "        oom = self.get_order_of_magnitude(value)\n",
    "        \n",
    "        #print(f'{oom=}')\n",
    "        \n",
    "        round_to_fifth = False\n",
    "        if oom >= 0:\n",
    "            # Positive Order of Mag. \n",
    "            if oom == 0:\n",
    "                round_int = 2\n",
    "            if oom == 1 :\n",
    "                round_int = 1\n",
    "                round_to_fifth = True\n",
    "            elif oom > 1:\n",
    "                round_int = 0\n",
    "                round_to_fifth = True\n",
    "        else:\n",
    "            # Negative Order of Mag.\n",
    "            if oom == -1:\n",
    "                round_int = 3\n",
    "            elif oom == -2:\n",
    "                round_int = 4    \n",
    "            else:\n",
    "                round_int = 4\n",
    " \n",
    "        # Round to the nearest 5 for >=10 \n",
    "        if round_to_fifth:\n",
    "            return round_to_nearest_fifth(round(value, round_int)), round_int\n",
    "        else:\n",
    "            return round(value, round_int), round_int\n",
    "\n",
    "        \n",
    "    def create_global(self, features, target):\n",
    "        \"\"\"Create the global explainability graphic\"\"\"\n",
    "        f, axes = plt.subplots(dpi=192, nrows=5, \n",
    "                           figsize=(800/192, 800/192))\n",
    "        for ax, feature in zip(axes, features):\n",
    "            self.create_local(feature, ax=ax, f=f)\n",
    "        \n",
    "        title = f\"\"\"\n",
    "        Training Set Distribution\\n(All {target}-Producing Storms)\\nfor the Top 5 Predictors (out of 113)\"\"\"\n",
    "    \n",
    "        f.suptitle(title, \n",
    "               fontsize=8, y=1.10)\n",
    "        \n",
    "        axes[0].set_title('Red numbers and vertical bars\\nshow current values for this object', \n",
    "                      fontsize=6, pad=12, color='red')\n",
    "        \n",
    "        plt.subplots_adjust(hspace=1.4)\n",
    "        \n",
    "        return f, axes \n",
    "                \n",
    "    def create_local(self, feature, target, f=None, ax=None):\n",
    "        \"\"\"Create box-and-whisker graphic for a single feature\"\"\"  \n",
    "        units = to_units(feature)\n",
    "        pretty_name = to_display_name(feature)\n",
    "    \n",
    "        if ax is None:\n",
    "            f, ax = plt.subplots(dpi=192, nrows=1, \n",
    "                           figsize=(800/192, 100/192))\n",
    "        \n",
    "        # Despine and only leave the bottom side. \n",
    "        for side in ['top', 'right', 'left']: \n",
    "            ax.spines[side].set_visible(False)\n",
    "\n",
    "        # Remove y tick labels \n",
    "        whis=[0.01, 99.9]\n",
    "        ax.tick_params(axis='x', labelsize=9, size=8)\n",
    "        \n",
    "        # Check for negative values. \n",
    "        min_val, round_int_min = self._round(np.nanpercentile(self.X_train[feature], whis[0]), 'lower')\n",
    "        max_val, round_int_max = self._round(np.nanpercentile(self.X_train[feature], whis[-1]), 'upper')\n",
    "        \n",
    "        round_int = round_int_min if self.is_negatively_oriented(self.X_train[feature]) else round_int_max\n",
    "        \n",
    "        self.max_min_val_dict[f'{feature}'] = {'round_int' : round_int}\n",
    "        \n",
    "        # Create the box-and-whiskers \n",
    "        box_plot = ax.boxplot(x=self.X_train[feature], vert=False, \n",
    "                              whis=whis, patch_artist=True, \n",
    "                              widths=0.3, showfliers=False )\n",
    "        ax.set_yticks([],)\n",
    "        \n",
    "        # Create a title for the feature name. \n",
    "        ax.annotate(fr'{pretty_name} ({units})', xy=(0, 1.15),\n",
    "                    xycoords='axes fraction', fontsize=6, ha='left', color = 'k', fontweight='bold')\n",
    "            \n",
    "        # Identify pretty tick ranges \n",
    "        ax.set_xlim(min_val, max_val)\n",
    "        \n",
    "        # set the tick locator for the x-axis\n",
    "        #ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=7))\n",
    "        \n",
    "        rng = list(ax.get_xticks())\n",
    "        \n",
    "        target_ = TARGET_DICT.get(target, target)\n",
    "        \n",
    "        \n",
    "        self.max_min_val_dict[f'{feature}_{target_}'] = {'max_val' : rng[-1],\n",
    "                                          'min_val' : rng[0], }\n",
    "        \n",
    "        # Identify pretty tick ranges \n",
    "        ax.set_xlim(rng[0], rng[-1])\n",
    "        \n",
    "        \n",
    "        if round_int in [0,1]:\n",
    "            labels = [f\"{int(round(v, round_int))}\" for v in rng]\n",
    "        else:\n",
    "            labels = [f\"{round(v, round_int)}\" for v in rng]\n",
    "        \n",
    "        labels[0] = ''; labels[-1] = ''\n",
    "        \n",
    "        high_val = np.max((abs(max_val), abs(min_val)))\n",
    "        #print(f'{rng=}  {labels=} {max_val=}  {min_val=}')\n",
    "        \n",
    "        ax.set_xticklabels(labels=labels, fontsize=self.get_fontsize(high_val))\n",
    "\n",
    "        # fill with colors\n",
    "        color = 'xkcd:medium blue'\n",
    "        for patch in box_plot['boxes']:\n",
    "            patch.set_facecolor(color)\n",
    "        for line in box_plot['medians']:\n",
    "            line.set_color('k')\n",
    "\n",
    "        return f, ax\n",
    "    \n",
    "    def save_local(self, fig, feature, target):\n",
    "        \n",
    "        target_ = TARGET_DICT.get(target, target)\n",
    "        \n",
    "        plt.savefig(\n",
    "            f\"new_graphics_2024/{feature}_{target_}_explainability_background.png\", \n",
    "            format=\"png\", dpi=192, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "        plt.close(fig) \n",
    "    \n",
    "    #def save_global(self, fig, target):    \n",
    "    #    plt.savefig(\n",
    "    #        f\"new_graphics/{target}_global_explainability_background.png\", \n",
    "    #        format=\"png\", dpi=192, bbox_inches=\"tight\", pad_inches=0.0)\n",
    "    #    plt.close(fig) \n",
    "    \n",
    "    def save_json(self, target): \n",
    "        \n",
    "        target_ = TARGET_DICT.get(target, target)\n",
    "        \n",
    "        with open(f\"../json/min_max_vals_{target_}.json\", \"w\") as outfile:\n",
    "            json.dump(self.max_min_val_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dd962",
   "metadata": {},
   "source": [
    "### Test a Single Individual Panel"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffef7808",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "target = 'hail_severe_0km'\n",
    "\n",
    "data = joblib.load(\n",
    "    '/work/mflora/ML_DATA/NEW_ML_MODELS/LogisticRegression_tornado_severe_0km_None_first_hour_realtime.joblib')\n",
    "features = data['X'].columns\n",
    "\n",
    "target_str = get_target_str(target)\n",
    "print(target_str.upper())\n",
    "df, y_train, metadata = load_ml_data(target, \n",
    "                 lead_time = 'first_hour', \n",
    "                 mode = 'training', \n",
    "                 baseline=False,\n",
    "                 return_only_df=False, \n",
    "                 load_reduced=True, \n",
    "                 base_path = '/work/mflora/ML_DATA/DATA',\n",
    "                )\n",
    "X_train = df[features]            \n",
    "        \n",
    "# Impute missing values. \n",
    "X_train = pd.DataFrame(SimpleImputer().fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "feature = 'ctt__time_min__amp_ens_mean_spatial_perc_10'\n",
    "\n",
    "explainer = cbWoFSExplainabilityGraphics(X_train.astype(float), y_train)\n",
    "fig, _ = explainer.create_local(feature, target_str)\n",
    "\n",
    "#explainer.save_local(fig, feature, target_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa54a9f",
   "metadata": {},
   "source": [
    "### Create Individual Panels for All Features Per Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0679dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Train Dates: 63...Num. of Val. Dates: 22\n",
      "4.406582575594905 4.5752950402121835\n",
      "np.any(np.isnan(X_val))=False\n",
      "np.any(np.isnan(y_val))=False\n",
      "np.any(np.isinf(X_val))=False\n",
      "np.any(np.isinf(y_val))=False\n"
     ]
    }
   ],
   "source": [
    "targets = [#'severe_mesh', \n",
    "           #'severe_wind', \n",
    "           #'severe_torn', \n",
    "          #['severe_mesh', 'severe_wind', 'severe_torn'],\n",
    "           #'any_sig_severe', \n",
    "           'hail_size'\n",
    "          ]\n",
    "\n",
    "TARGET_DICT = { 'severe_mesh' : 'hail',\n",
    "                'severe_wind' : 'wind',\n",
    "                'severe_torn' : 'tornado', \n",
    "                'any_sig_severe' : 'all_sig_severe', \n",
    "               'hail_size' : 'hail_size'\n",
    "              }\n",
    "\n",
    "def get_target_str(target):\n",
    "    if isinstance(target, list):\n",
    "        return 'all_severe'\n",
    "    else:\n",
    "        return TARGET_DICT[target]\n",
    "\n",
    "for target in targets:\n",
    "    if target == 'hail_size': \n",
    "        time = 'first_hour'\n",
    "        loader = HailSizeLoader(time)\n",
    "        X_train, _, y_train, _ = loader.load('training')\n",
    "    else:\n",
    "        loader_kws= {\n",
    "              'data_path' :  '/work/mflora/ML_DATA/DATA/',\n",
    "              'return_full_dataframe': False, \n",
    "              'random_state' : 123, \n",
    "               'mode' : 'testing',\n",
    "               'exclude_missing_mesh': True\n",
    "        }\n",
    "        time = ['first_hour', 'second_hour', 'third_hour', 'fourth_hour'] \n",
    "\n",
    "        loader = MLTrainer(loader_kws=loader_kws) \n",
    "        X_train, y_train, metadata_train = loader.load_data('Model', target, time)\n",
    "             \n",
    "    # Impute missing values. \n",
    "    X_train = pd.DataFrame(SimpleImputer().fit_transform(X_train), columns=X_train.columns)\n",
    "        \n",
    "    explainer = cbWoFSExplainabilityGraphics(X_train.astype(float), y_train)\n",
    "    \n",
    "    target_str = get_target_str(target)\n",
    "    \n",
    "    for feature in X_train.columns: \n",
    "        fig, _ = explainer.create_local(feature, target_str)\n",
    "        explainer.save_local(fig, feature, target_str)\n",
    "        \n",
    "    explainer.save_json(target_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261d6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the jsons together! \n",
    "from glob import glob\n",
    "files = glob(\"../json/min_max_vals*\")\n",
    "pd.read_json(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23819d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_preds = {'tornado_severe_0km' : \n",
    "            ['wz_0to2_instant__time_max__amp_ens_mean_spatial_perc_90', \n",
    "             'shear_v_0to6__ens_mean__spatial_mean',\n",
    "             'buoyancy__time_min__amp_ens_mean_spatial_perc_10',\n",
    "             '10-500m_bulkshear__time_max__amp_ens_mean_spatial_perc_90',\n",
    "             'v_10__ens_mean__spatial_mean',\n",
    "            ],\n",
    "            'hail_severe_0km' : \n",
    "                    [\n",
    "              'dbz_3to5__time_max__ens_mean__spatial_mean',\n",
    "              'comp_dz__time_max__amp_ens_mean_spatial_perc_90',  \n",
    "              'td_850__ens_mean__spatial_mean',\n",
    "              '10-500m_bulkshear__time_max__ens_mean__spatial_mean',\n",
    "              'w_down__time_min__amp_ens_mean_spatial_perc_10'\n",
    "    \n",
    "            ],\n",
    "            'wind_severe_0km' : \n",
    "                    [\n",
    "                  'v_10__ens_mean__spatial_mean',\n",
    "                  'ws_80__time_max__amp_ens_mean_spatial_perc_90',\n",
    "                  'comp_dz__time_max__amp_ens_mean_spatial_perc_90',\n",
    "                  'div_10m__time_min__ens_mean__spatial_mean',\n",
    "                  'buoyancy__time_min__amp_ens_mean_spatial_perc_10',  \n",
    "                    ],\n",
    "             \n",
    "             'all_severe' : \n",
    "             [\n",
    "                'comp_dz__time_max__amp_ens_mean_spatial_perc_90',\n",
    "                'div_10m__time_min__ens_std__spatial_mean',\n",
    "                'ctt__time_min__amp_ens_mean_spatial_perc_10',\n",
    "                'hailcast__time_max__ens_mean__spatial_mean',\n",
    "                '10-500m_bulkshear__time_max__ens_mean__spatial_mean',\n",
    "                ],\n",
    "             'all_sig_severe': [\n",
    "                'low_level_lapse_rate__ens_mean__spatial_mean',\n",
    "                'ctt__time_min__amp_ens_mean_spatial_perc_10',\n",
    "                '10-500m_bulkshear__time_max__ens_mean__spatial_mean',\n",
    "                'hailcast__time_max__ens_mean__spatial_mean',\n",
    "                'comp_dz__time_max__amp_ens_mean_spatial_perc_90', \n",
    "                ],\n",
    "             \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, metadata = load_ml_data('wind_severe_0km', \n",
    "                 lead_time = 'first_hour', \n",
    "                 mode = None, \n",
    "                 baseline=False,\n",
    "                 return_only_df=False, \n",
    "                 load_reduced=True, \n",
    "                 base_path = '/work/mflora/ML_DATA/DATA',\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = top_preds['wind_severe_0km']\n",
    "\n",
    "dataframe = X_train.iloc[:10, :]\n",
    "\n",
    "n_examples = len(dataframe)\n",
    "\n",
    "top_values = dataframe[top_features].values \n",
    "top_features_list = [list(top_features) for _ in range(n_examples)]\n",
    "\n",
    "val_df = pd.DataFrame(top_values, columns=[f'Feature Val {i+1}' for i in range(5)])\n",
    "feature_df = pd.DataFrame(top_features_list, columns=[f'Feature Name {i+1}' for i in range(5)])\n",
    "    \n",
    "total_df = pd.concat([val_df, feature_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c022ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_values.shape, n_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb8c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21565471",
   "metadata": {},
   "source": [
    "### Create the top 5 predictor global panel "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4df56315",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "\n",
    "TITLES = {'wind_severe_0km' : 'Severe Wind', \n",
    "          'hail_severe_0km' : 'Severe Hail', \n",
    "          'tornado_severe_0km' : 'Tornado', \n",
    "          'all_severe' : 'Any Severe', \n",
    "          'all_sig_severe' : 'Any Sig. Severe', \n",
    "         }\n",
    "\n",
    "\n",
    "targets = ['wind_severe_0km', \n",
    "           'hail_severe_0km', \n",
    "           'tornado_severe_0km', \n",
    "           ['wind_severe_0km', 'hail_severe_0km', 'tornado_severe_0km'],\n",
    "           ['wind_sig_severe_0km', 'hail_sig_severe_0km', 'tornado_sig_severe_0km']\n",
    "          ]\n",
    "\n",
    "for target in targets: \n",
    "    target_str = get_target_str(target)\n",
    "    X_train, y_train, metadata = load_ml_data(target, \n",
    "                 lead_time = 'first_hour', \n",
    "                 mode = None, \n",
    "                 baseline=False,\n",
    "                 return_only_df=False, \n",
    "                 load_reduced=True, \n",
    "                 base_path = '/work/mflora/ML_DATA/DATA',\n",
    "                )\n",
    "\n",
    "    # Impute missing values. \n",
    "    X_train = pd.DataFrame(SimpleImputer().fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "\n",
    "    explainer = cbWoFSExplainabilityGraphics(X_train.astype(float), y_train)\n",
    "    features = top_preds[target_str]\n",
    "    fig, _ = explainer.create_global(features, target=TITLES[target_str])\n",
    "    \n",
    "    explainer.save_global(fig, target_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
